#! /usr/bin/env python3
"""
paper_collecter script

collects papers from rss feeds and stores them in the database
"""

# % IMPORTS

import json
import feedparser as fp
from pathlib import Path

# % LOCAL IMPORT

from sql_connect import sqlconnector

# % SETTINGS

DIR = Path(__file__).parents[0]  # returns the script directory
DB_PATH = DIR / ".." / "db" / "paperwatch.sqlite"

FEED_EXAMPLE_PATH = DIR / ".." / "example_feeds"
FEED_EXAMPLE = "physics.atom-ph"
USE_FEED_EXAMPLE = True


# % FUNCTIONS


@sqlconnector(DB_PATH)
def load_feeds(db_con):
    # -- prepare sql request
    sql_req = f"select * from feeds;"
    cur = db_con.execute(sql_req)
    # -- store results in dictionnary
    fields = [x[0] for x in cur.description]
    feeds = []
    for row in cur:
        new_feed = {k: v for k, v in zip(fields, row)}
        feeds.append(new_feed)
    return feeds


def get_papers_from_rss(feed):
    # -- read rss flux
    data = fp.parse(feed)
    papers = data["entries"]
    # -- get in good format
    if papers != []:
        while not isinstance(papers[0], dict):
            papers = papers[0]
    return papers


def collect_papers(feeds_list):
    """
    reads rss flux and return a list of papers
    """
    # -- prepare output
    collection = []

    # -- read feeds
    for feed in feeds_list:
        result = {"feed": feed}
        try:
            feed_papers = get_papers_from_rss(feed["url"])
        except Exception as e:
            print("An error occured, let's go on...")
            print(e)
            feed_papers = []
        result["papers_raw"] = feed_papers
        collection.append(result)

    return collection


def _get_authors_list(value, type):
    """
    parse author list, depending on the type
    """
    implemented_types = [
        "cs-list",
    ]
    assert type in implemented_types, f"implemented author types = {implemented_types}"
    match type:
        case "cs-list":
            aut_list = value.split(",")
            aut_list = [aut.strip() for aut in aut_list]
            return aut_list


def _get_tag_list(value, type):
    """
    parse tag list, depending on the type
    """
    implemented_types = [
        "arxiv",
    ]
    assert type in implemented_types, f"implemented tag types = {implemented_types}"
    match type:
        case "arxiv":
            tag_list = [t["term"] for t in value]
            return tag_list


def parse_papers(result):
    """
    takes the result for a feed as yielded by `collect_papers`
    and convert it in a form that is ready to be stored
    in the database
    """
    # -- get feed info and raw paper list
    feed = result["feed"]
    papers_raw = result["papers_raw"]

    # -- loop on papers
    papers = []
    for p in papers_raw:
        pc = {}
        # 'easy' fields
        for name in ["link", "title", "summary", "id"]:
            feed_field = feed.get(f"field_{name}")
            pc[name] = p.get(feed_field)
        # authors
        authors_value = p.get(feed.get("field_author"))
        authors_type = feed.get("field_author_type")
        pc["authors"] = _get_authors_list(authors_value, authors_type)
        # tags
        tags_value = p.get(feed.get("field_tags"))
        tags_type = feed.get("field_tags_type")
        pc["tags"] = _get_tag_list(tags_value, tags_type)
        # append
        papers.append(pc)

    # -- store result
    result["papers"] = papers

@sqlconnector(DB_PATH)
def store_papers(db_con, collection, harvest_id):
    """
    Take collected papers and insert them into the database
    """


@sqlconnector(DB_PATH)
def create_harvest_entry(db_con, feeds):
    """
    creates an entry in the harvest table
    and returns the id
    """
    # -- prepare request
    feed_info = [{k:f[k] for k in ["id", "display_name"]} for f in feeds]
    feed_info_json = json.dumps(feed_info)
    sql_req = f"INSERT INTO harvest (feeds) values ('{feed_info_json}');"
    # -- insert new entry with feed info
    db_con.execute(sql_req)
    # -- get last entry
    res = db_con.execute("select max(id) from harvest;")
    last_id = res.fetchone()[0]
    return last_id


# % FOR DEBUGGING


def collect_papers_from_example():
    """
    do not connect to an actual rss flux
    but use a feed stored in "../example_feeds" instead
    """
    # -- prepare output
    collection = []

    # -- read feed
    # create a mockup feed
    feed = {
        "id": -1,
        "url": "http://mockup.feed/rss",
        "display_name": "mockup",
        "category": "debug",
        "field_date": "published_parsed",
        "field_title": "title",
        "field_link": "link",
        "field_author": "author",
        "field_summary": "summary",
        "field_id": "id",
        "field_tags": "tags",
        "field_author_type": "cs-list",
        "field_tags_type": "arxiv",
    }
    # read
    example_feed = FEED_EXAMPLE_PATH / FEED_EXAMPLE
    with example_feed.open() as f:
        papers = get_papers_from_rss(f.read())
    # result
    result = {"feed": feed, "papers_raw": papers}
    collection.append(result)

    return collection


# % MAIN ROUTINE


def main():
    # -- 1 - get feeds settings
    # get information about feeds
    feeds = load_feeds()
    # -- 2 - add a harvest entry
    harvest_id = create_harvest_entry(feeds)
    # -- 3 - collect papers
    # for each feed, collect raw entries
    if USE_FEED_EXAMPLE:
        collection = collect_papers_from_example()
    else:
        collection = collect_papers(feeds)
    # -- 4 - parse papers
    # each result is modified in place
    # and a 'papers' field is added to the result dict
    for res in collection:
        parse_papers(res)
    # -- 5 - store papers in database
    store_papers(collection, harvest_id)
    # ----------- DEBUG --------------
    res = collection[0]
    pp = res["papers"][0]
    print([(k, v) for k, v in pp.items()])
    # print(res["papers_rabw"][0]["tags"])


# % EXECUTION
if __name__ == "__main__":
    main()
