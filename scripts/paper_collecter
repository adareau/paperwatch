#! /usr/bin/env python3
"""
paper_collecter script

collects papers from rss feeds and stores them in the database
"""

# % IMPORTS

import json
import re
import feedparser as fp
from pathlib import Path
from time import mktime
from datetime import datetime

# % LOCAL IMPORT

from sql_connect import sqlconnector

# % SETTINGS

DIR = Path(__file__).parents[0]  # returns the script directory
DB_PATH = DIR / ".." / "db" / "paperwatch.sqlite"

FEED_EXAMPLE_PATH = DIR / ".." / "example_feeds"
FEED_EXAMPLE = "physics.atom-ph"
USE_FEED_EXAMPLE = False


# % FUNCTIONS


@sqlconnector(DB_PATH)
def load_feeds(db_con):
    write_log("harvest", "load feeds")
    # -- prepare sql request
    sql_req = f"select * from feeds;"
    cur = db_con.execute(sql_req)
    # -- store results in dictionnary
    fields = [x[0] for x in cur.description]
    feeds = []
    for row in cur:
        new_feed = {k: v for k, v in zip(fields, row)}
        feeds.append(new_feed)
    return feeds


def get_papers_from_rss(feed):
    # -- read rss flux
    data = fp.parse(feed)
    papers = data["entries"]
    # -- get in good format
    if papers != []:
        while not isinstance(papers[0], dict):
            papers = papers[0]
    return papers


def collect_papers(feeds_list):
    """
    reads rss flux and return a list of papers
    """
    write_log("harvest", "collect papers")
    # -- prepare output
    collection = []

    # -- read feeds
    for feed in feeds_list:
        result = {"feed": feed}
        try:
            feed_papers = get_papers_from_rss(feed["url"])
        except Exception as e:
            print("An error occured, let's go on...")
            print(e)
            feed_papers = []
        result["papers_raw"] = feed_papers
        collection.append(result)

    return collection


def _get_authors_list(value, type):
    """
    parse author list, depending on the type
    """
    implemented_types = [
        "cs-list",
    ]
    assert type in implemented_types, f"implemented author types = {implemented_types}"
    match type:
        case "cs-list":
            aut_list = value.split(",")
            aut_list = [aut.strip() for aut in aut_list]
            regex = re.compile(r"[^a-zA-Z \.-]")
            aut_list = [regex.sub("", aut) for aut in aut_list]
            aut_list = json.dumps(aut_list)
            return aut_list


def _get_tag_list(value, type):
    """
    parse tag list, depending on the type
    """
    implemented_types = [
        "arxiv",
    ]
    assert type in implemented_types, f"implemented tag types = {implemented_types}"
    match type:
        case "arxiv":
            tag_list = [t["term"] for t in value]
            tag_list = json.dumps(tag_list)
            return tag_list


def _get_date(value, type):
    """
    parse date
    """
    implemented_types = [
        "timestruct",
    ]
    assert (
        type in implemented_types
    ), f"type '{type}' unknown : implemented date types = {implemented_types}"
    match type:
        case "timestruct":
            date = datetime.fromtimestamp(mktime(value))
            date = date.strftime("%Y-%m-%d %H:%M:%S")
            return date


def parse_papers(result, harvest_id):
    """
    takes the result for a feed as yielded by `collect_papers`
    and convert it in a form that is ready to be stored
    in the database
    """
    write_log("harvest", "parse papers")
    # -- get feed info and raw paper list
    feed = result["feed"]
    papers_raw = result["papers_raw"]

    # -- loop on papers
    papers = []
    for p in papers_raw:
        pc = {}
        # 'easy' fields
        for name in ["link", "title", "summary", "paper_id", "doi"]:
            feed_field = feed.get(f"field_{name}")
            pc[name] = p.get(feed_field)
        # authors
        authors_value = p.get(feed.get("field_author"))
        authors_type = feed.get("field_author_type")
        pc["author"] = _get_authors_list(authors_value, authors_type)
        # tags
        tags_value = p.get(feed.get("field_tags"))
        tags_type = feed.get("field_tags_type")
        pc["feed_tags"] = _get_tag_list(tags_value, tags_type)
        # date
        date_value = p.get(feed.get("field_date"))
        date_type = feed.get("field_date_type")
        pc["date_published"] = _get_date(date_value, date_type)
        # feed
        pc["feed_url"] = feed["url"]
        pc["feed_id"] = feed["id"]
        # harvest
        pc["harvest_id"] = harvest_id
        # append
        papers.append(pc)

    # -- store result
    result["papers"] = papers


def _is_paper_in_db(db_con, link):
    sql_req = f"select link from papers where link='{link}';"
    cur = db_con.execute(sql_req)
    res = cur.fetchall()
    return len(res)


def _insert_paper_in_db(db_con, p):
    # -- prepare request
    req_pattern = "INSERT INTO papers({}) values ({});"
    values = ",".join(["?" for value in p.values()])
    names = ",".join(p.keys())
    req = req_pattern.format(names, values)
    # -- post
    try:
        db_con.execute(req, list(p.values()))
    except Exception as e:
        print(e)
        return 0
    return 1


@sqlconnector(DB_PATH)
def store_papers(db_con, collection):
    """
    Take collected papers and insert them into the database
    """
    write_log("harvest", "store papers")
    # -- prepare counter
    papers_collected = 0
    papers_collected_new = 0
    papers_errors = 0

    # -- iterate
    for res in collection:
        # - get data
        papers = res["papers"]
        papers_collected += len(papers)
        # - loop on papers
        for p in papers:
            if _is_paper_in_db(db_con, p["link"]):
                continue
            papers_collected_new += 1
            if not _insert_paper_in_db(db_con, p):
                papers_errors += 1

    # -- return result
    res = {}
    res["papers_collected"] = papers_collected
    res["papers_collected_new"] = papers_collected_new
    res["papers_errors"] = papers_errors
    res["success"] = True
    return res


@sqlconnector(DB_PATH)
def create_harvest_entry(db_con, feeds):
    """
    creates an entry in the harvest table
    and returns the id
    """
    write_log("harvest", "create harvest entry")
    # -- prepare request
    feed_info = [{k: f[k] for k in ["id", "display_name"]} for f in feeds]
    feed_info_json = json.dumps(feed_info)
    sql_req = f"INSERT INTO harvest (feeds) values ('{feed_info_json}');"
    # -- insert new entry with feed info
    db_con.execute(sql_req)
    # -- get last entry
    res = db_con.execute("select max(id) from harvest;")
    last_id = res.fetchone()[0]
    return last_id

@sqlconnector(DB_PATH)
def store_harvest_results(db_con, result, harvest_id):
    """
    store harvest results in the harvest table
    """
    write_log("harvest", "store harvest result")
    req = "UPDATE harvest SET {} WHERE id = ?;"
    values_str = ",".join([f"{k} = ?" for k in result.keys()])
    values = list(result.values()) + [harvest_id, ]
    req = req.format(values_str)
    db_con.execute(req, values)



# % FOR DEBUGGING


def collect_papers_from_example():
    """
    do not connect to an actual rss flux
    but use a feed stored in "../example_feeds" instead
    """
    write_log("harvest", "collect papers (dummy)")
    # -- prepare output
    collection = []

    # -- read feed
    # create a mockup feed
    feed = {
        "id": 0,
        "url": "http://mockup.feed/rss",
        "display_name": "mockup",
        "category": "debug",
        "field_date": "published_parsed",
        "field_date_type": "timestruct",
        "field_title": "title",
        "field_link": "link",
        "field_author": "author",
        "field_summary": "summary",
        "field_paper_id": "id",
        "field_tags": "tags",
        "field_author_type": "cs-list",
        "field_tags_type": "arxiv",
        "field_doi": "doi",
    }
    # read
    example_feed = FEED_EXAMPLE_PATH / FEED_EXAMPLE
    with example_feed.open() as f:
        papers = get_papers_from_rss(f.read())
    # result
    result = {"feed": feed, "papers_raw": papers}
    collection.append(result)

    return collection


@sqlconnector(DB_PATH)
def write_log(db_con, action, details="", error=False):
    db_con.execute("INSERT INTO log(action, details, error) values(?, ?, ?)", (action, details, error))


# % MAIN ROUTINE


def main():
    write_log("harvest", "--- starting harvest ---")
    # -- 1 - get feeds settings
    # get information about feeds
    feeds = load_feeds()
    # -- 2 - add a harvest entry
    harvest_id = create_harvest_entry(feeds)
    # -- 3 - collect papers
    # for each feed, collect raw entries
    if USE_FEED_EXAMPLE:
        collection = collect_papers_from_example()
    else:
        collection = collect_papers(feeds)
    # -- 4 - parse papers
    # each result is modified in place
    # and a 'papers' field is added to the result dict
    for res in collection:
        parse_papers(res, harvest_id)
    # -- 5 - store papers in database
    result = store_papers(collection)
    # -- 6 - write result
    store_harvest_results(result, harvest_id)
    write_log("harvest", "--- success ---")


# % EXECUTION
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        write_log("harvest", str(e), error=True)
        raise e
