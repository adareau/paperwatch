#! /usr/bin/env python3
# -*- coding: utf-8 -*-
"""
File    : paperwatch
Author  : A. Dareau

Comments: paperwatch routine
"""
# %% Imports

import urllib
import sqlite3
import feedparser as fp
import re
import webbrowser
import os
import configparser
import sys
import json

from pathlib import Path
from datetime import datetime, timedelta
from time import mktime

import html_style as html

# %% Global Variables

# Configuration
CONF_PATH = os.path.join("..", "config", "%s.conf")
DB_PATH = Path(".") / "db" / "paperwatch.sqlite"
KEYWORDS = {}
DAYS = 8
SETTINGS = {}

# Save options
INDIV_REPORT_FOLDER = "individual_reports"
JSON_FOLDER = "json"
DATE_FMT = "%Y_%m_%d"

# Global collections
ALL_PAPERS = {}
SELECTED_PAPERS = {}

# Hardcoded settings
QUIET = False
DEBUG = []
TEST_RUN = False

# Test run option
if len(sys.argv) > 1:
    mode = sys.argv[1]
    if mode == "test":
        TEST_RUN = True

# %% Side functions - subroutines

# -- Text cleaning and formatting


def _cleanhtml(raw_html):
    cleanr = re.compile("<.*?>")
    cleantext = re.sub(cleanr, "", raw_html)
    return cleantext


def _convtohtml(unicode_str):
    # out = unicode_str.encode("ascii", "xmlcharrefreplace")
    # TODO : remove this function !
    # useless in python3 .
    return unicode_str


def _cleantitle(raw_title):
    cleanr = re.compile("\(arXiv:.*?].*?\)")
    cleantext = re.sub(cleanr, "", raw_title)
    return cleantext


def _explode_and_trim(str_in):
    if str_in is None:
        return [""]
    list_out = str_in.split(",")
    list_out = [e.strip() for e in list_out]
    return list_out


def _qprint(strg):
    global QUIET
    if not QUIET:
        print(strg)


def _prdelim():
    _qprint("-" * 30)


def _prsubdelim():
    _qprint("-" * 15)


# -- DB connection


def connect_to_database(db_path):
    if not db_path.exists():
        raise FileExistsError(
            f"Cannot find database '{db_path.resolve()}': file does not exist"
        )
    connexion = sqlite3.connect(db_path)
    return connexion


# -- Setting handling


def load_keywords(db_con):
    # -- prepare sql request
    fields = ["category", "short_name", "keywords"]
    sql_req = f"select {",".join(fields)} from keywords;"
    cur = db_con.cursor()
    # -- store results in dictionnary
    keywords = []
    for row in cur.execute(sql_req):
        new_cat = {k: v for k, v in zip(fields, row)}
        new_cat["keywords"] = _explode_and_trim(new_cat["keywords"])
        keywords.append(new_cat)
    return keywords


def load_authors(db_con):
    # -- prepare sql request
    fields = ["name", "tags"]
    sql_req = f"select {",".join(fields)} from authors;"
    cur = db_con.cursor()
    # -- store results in dictionnary
    authors = []
    for row in cur.execute(sql_req):
        new_auth = {k: v for k, v in zip(fields, row)}
        new_auth["tags"] = _explode_and_trim(new_auth["tags"])
        authors.append(new_auth)
    return authors


def load_feeds(db_con):
    # -- prepare sql request
    fields = ["url", "display_name", "category", "save_all", "check_date"]
    sql_req = f"select {",".join(fields)} from feeds;"
    cur = db_con.cursor()
    # -- store results in dictionnary
    feeds = []
    for row in cur.execute(sql_req):
        new_feed = {k: v for k, v in zip(fields, row)}
        feeds.append(new_feed)
    return feeds


def load_settings():
    global CONF_PATH, SETTINGS
    # -- parse config file
    cp = configparser.ConfigParser()
    cp.read(CONF_PATH % "settings")
    # -- populate dictionnary
    SETTINGS = {}
    SETTINGS["output"] = {"out_directory": cp.get("output", "out_directory")}


# %% Core functions

# -- Get and process papers


def get_papers_from_rss(feed):
    # -- read rss flux
    url = feed["url"]
    raw_data = urllib.request.urlopen(url).read()
    data = fp.parse(raw_data)
    papers = data["entries"]

    # -- get in good format
    if papers != []:
        while not isinstance(papers[0], dict):
            papers = papers[0]

    # -- keep recent papers if needed
    if feed["check_date"]:
        papers = keep_recent_papers(papers)
        pass

    return papers


def keep_recent_papers(papers):
    global DAYS
    now_date = datetime.now()
    day_start = datetime(now_date.year, now_date.month, now_date.day)
    start_date = day_start - timedelta(days=DAYS)
    recent_papers = []
    for p in papers:
        if "updated_parsed" in p:
            date = p["updated_parsed"]
            date = datetime.fromtimestamp(mktime(date))
            if date >= start_date:
                recent_papers.append(p)
    return recent_papers


def _paper_check(p):
    """
    takes one paper as input, check whether we keep it !
    """
    # check that this is a signed paper
    # (some feeds have news which are not papers, with no authors)
    if "authors" not in p:
        return False

    # title = _convtohtml(p['title'])
    # authors = _convtohtml(_cleanhtml(p['author']))
    scores = {}
    total_score = 0
    # keywords
    for cat in KEYWORDS.keys():
        scores[cat] = 0
        fields = KEYWORDS[cat]["fields"]
        content = " ".join([_convtohtml(_cleanhtml(p[f])) for f in fields])
        for k in KEYWORDS[cat]["keywords"]:
            if k.lower() in content.lower():
                scores[cat] += 1
                total_score += 1
    if total_score > 0:
        p["score"] = scores
        p["total_score"] = total_score
        return p

    else:
        return False


def paper_harvester():
    """
    main routine for paper harvesting : reads rss flux, select interesting
    papers and return everything for saving.
    """
    global ALL_PAPERS, SELECTED_PAPERS, FEEDS, DEBUG

    # Reload settings
    load_keywords()
    load_feeds()

    # Reset paper collections
    ALL_PAPERS = {}
    SELECTED_PAPERS = {}

    # Harvest
    _prdelim()
    _qprint("  Starting Harvest")

    for cat in FEEDS.keys():
        _prdelim()
        _qprint(">> Category : %s" % cat)
        feed_dic = FEEDS[cat]
        ALL_PAPERS[cat] = {}
        SELECTED_PAPERS[cat] = {}
        for name, feed in feed_dic.items():
            _prsubdelim()
            _qprint("> %s" % name)
            try:
                feed_papers = get_papers_from_rss(feed)
                selection = []
                for paper in feed_papers:
                    selected = _paper_check(paper)
                    if selected:
                        selection.append(selected)

                ALL_PAPERS[cat][name] = feed_papers
                SELECTED_PAPERS[cat][name] = selection
                Npapers = len(feed_papers)
                Nselected = len(selection)
                _qprint("- Selected papers : %i of %i" % (Nselected, Npapers))
            except Exception as e:
                print("An error occured, let's go on...")
                print(e)
                ALL_PAPERS[cat][name] = []
                SELECTED_PAPERS[cat][name] = []


# -- Generating HTML outputs


def generate_special_report():
    """
    special selection based on score and authors
    """
    global SELECTED_PAPERS, INDIV_REPORT_FOLDER, SETTINGS, DATE_FMT
    # prepare lists
    n_high_score = 0
    high_scores = {}
    known_authors = []

    # analyze all papers
    for source in SELECTED_PAPERS.keys():
        for cat in SELECTED_PAPERS[source].keys():
            for p in SELECTED_PAPERS[source][cat]:
                # store high scores
                tot_score = p["total_score"]
                if tot_score > 1:
                    if tot_score not in high_scores:
                        high_scores[tot_score] = []
                    high_scores[tot_score].append(p)
                    high_scores[tot_score][-1]["SOURCE"] = cat
                    n_high_score += 1

                # store authors
                if p["score"]["Authors"] > 0:
                    known_authors.append(p)
                    known_authors[-1]["SOURCE"] = cat

    n_authors = len(known_authors)

    # -- Generate output
    date_header = datetime.now().strftime("%d-%m-%Y")
    # header
    output = html.cat_header % ("[special]", date_header)
    # high scores
    for score in sorted(list(high_scores.keys())):
        output += html.cat_feedname % ("â˜…" * score)
        for p in high_scores[score]:
            title = _cleantitle(_convtohtml(p["title"]))
            authors = _convtohtml(p["author"])
            score = p["total_score"]
            kw_str = ""
            for k, s in p["score"].items():
                if s > 0:
                    kw_str += k + ", "
            kw_str += " [%s]" % p["SOURCE"]
            url = p["link"]
            try:
                date = p["updated_parsed"]
                date = datetime.fromtimestamp(mktime(date))
                date = datetime.strftime(date, "%d/%m/%Y")
            except:
                date = "unknown"

            new_entry = html.selected_entry % (
                url,
                title,
                authors,
                date,
                score,
                kw_str,
            )
            output += new_entry
        output += html.cat_feedfooter

    # authors
    output += html.cat_feedname % "authors"
    for p in known_authors:
        title = _cleantitle(_convtohtml(p["title"]))
        authors = _convtohtml(p["author"])
        score = p["total_score"]
        kw_str = ""
        for k, s in p["score"].items():
            if s > 0:
                kw_str += k + ", "
        kw_str += " [%s]" % p["SOURCE"]
        url = p["link"]
        try:
            date = p["updated_parsed"]
            date = datetime.fromtimestamp(mktime(date))
            date = datetime.strftime(date, "%d/%m/%Y")
        except:
            date = "unknown"

        new_entry = html.selected_entry % (
            url,
            title,
            authors,
            date,
            score,
            kw_str,
        )
        output += new_entry
    output += html.cat_feedfooter
    # -- Generate file
    out_path_root = SETTINGS["output"]["out_directory"]
    out_path = os.path.join(out_path_root, INDIV_REPORT_FOLDER)
    date_str = datetime.strftime(datetime.now(), DATE_FMT)
    fname = date_str + "_special.html"
    fpath = os.path.join(out_path, fname)

    with open(fpath, "w") as f:
        f.write(output)

    return fname, n_high_score, n_authors


def generate_individual_report(category):
    """
    generates an individual report for one feed category
    category : string, key for the selected category
    """
    global FEEDS, SETTINGS, INDIV_REPORT_FOLDER, ALL_PAPERS, SELECTED_PAPERS, DATE_FMT, TEST_RUN

    # -- Get feed options and papers
    feed = FEEDS[category]
    selection = SELECTED_PAPERS[category]
    papers = ALL_PAPERS[category]

    # -- Configuration
    out_path_root = SETTINGS["output"]["out_directory"]
    out_path = os.path.join(out_path_root, INDIV_REPORT_FOLDER)
    date_str = datetime.strftime(datetime.now(), DATE_FMT)
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    date_header = datetime.now().strftime("%d-%m-%Y")

    # -- Generate output
    # header
    output = html.cat_header % (category, date_header)

    # generate summary
    output += "<h2> summary </h2> \n"
    output += "RSS flux read on %s <br/>" % now_str
    output += "<ul> \n"
    for name in feed:
        Nsel = len(selection[name])
        Ntot = len(papers[name])
        feed_url = feed[name]["url"]
        if Nsel > 0:
            output += "<b>%s</b>" % html.summary % (feed_url, name, Nsel, Ntot)
        else:
            output += html.summary % (feed_url, name, Nsel, Ntot)

    output += "</ul> \n"

    # generate article list (SELECTION)
    for name, settings in feed.items():
        paper_list = selection[name]
        if len(paper_list) > 0:
            output += html.cat_feedname % settings["display_name"]
            for p in paper_list:
                title = _cleantitle(_convtohtml(p["title"]))
                authors = _convtohtml(p["author"])
                score = p["total_score"]
                kw_str = ""
                for k, s in p["score"].items():
                    if s > 0:
                        kw_str += k + ", "
                url = p["link"]
                try:
                    date = p["updated_parsed"]
                    date = datetime.fromtimestamp(mktime(date))
                    date = datetime.strftime(date, "%d/%m/%Y")
                except:
                    date = "unknown"

                new_entry = html.selected_entry % (
                    url,
                    title,
                    authors,
                    date,
                    score,
                    kw_str,
                )
                output += new_entry
            output += html.cat_feedfooter

    # generate article list (for fully saved categories)
    output += "<hr/> \n <h2> Comprehensive list </h2>"
    for name, settings in feed.items():
        if settings["save_all"]:
            output += "<h3>%s</h3> \n <ul> \n" % settings["display_name"]
            paper_list = papers[name]
            for p in paper_list:
                title = _cleantitle(p["title"])
                authors = p["author"]
                url = p["link"]
                output += html.small_entry % (url, title, authors)
            output += "</ul> \n"

    output += html.cat_footer

    # -- Generate file
    if TEST_RUN:
        fname = date_str + "_test" + "_%s.html" % category
    else:
        fname = date_str + "_%s.html" % category

    fpath = os.path.join(out_path, fname)

    with open(fpath, "w") as f:
        f.write(output)

    return fpath, fname


def generate_html_output():
    """
    generates an html summary with links to individual reports
    as well as all individual reports
    """
    global FEEDS, SETTINGS, INDIV_REPORT_FOLDER, ALL_PAPERS, SELECTED_PAPERS, DATE_FMT, TEST_RUN

    # -- Configuration
    out_path_root = SETTINGS["output"]["out_directory"]
    out_path_indiv = os.path.join(".", INDIV_REPORT_FOLDER)

    date_str = datetime.strftime(datetime.now(), DATE_FMT)
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    date_header = datetime.now().strftime("%d-%m-%Y")

    # -- Generate output
    # header
    _prdelim()
    _qprint("Generating files")
    output = html.main_header % (date_header)
    output += "RSS flux read on %s <br/>" % now_str

    # special cat
    fname, n_high_score, n_authors = generate_special_report()
    url = os.path.join(out_path_indiv, fname)
    output += html.main_feedname % (url, "[special]")
    output += "Selection : %i high scores, %i  authors\n" % (
        n_high_score,
        n_authors,
    )

    # category reports and summary
    for category in FEEDS_ORDER:
        # generate the report
        _qprint(">> %s" % category)
        _, fname = generate_individual_report(category)
        url = os.path.join(out_path_indiv, fname)
        # Get feed options and papers
        feed = FEEDS[category]
        selection = SELECTED_PAPERS[category]
        papers = ALL_PAPERS[category]
        output += html.main_feedname % (url, category)
        output_buffer = ""
        Nsel_tot = 0
        Ntot_tot = 0
        for name in feed:
            Nsel = len(selection[name])
            Ntot = len(papers[name])
            feed_url = feed[name]["url"]
            Nsel_tot += Nsel
            Ntot_tot += Ntot
            if Nsel > 0:
                output_buffer += (
                    "<b>%s</b>" % html.summary % (feed_url, name, Nsel, Ntot)
                )
            else:
                output_buffer += html.summary % (feed_url, name, Nsel, Ntot)

        output += "Selection : %i papers out of %i \n <ul>" % (
            Nsel_tot,
            Ntot_tot,
        )
        output += output_buffer
        output += html.main_feedfooter

    output += html.cat_footer

    # -- Generate file
    if TEST_RUN:
        fname = date_str + "_test" + ".html"
    else:
        fname = date_str + ".html"
    fpath = os.path.join(out_path_root, fname)

    with open(fpath, "w") as f:
        f.write(output)

    return fpath


# -- SAVE JSON


def save_as_json():
    """
    saves the SELECTED_PAPERS dictionnary as a json file
    """
    global SELECTED_PAPERS, DATE_FMT, JSON_FOLDER, SETTINGS

    # generate file path and name
    out_path_root = SETTINGS["output"]["out_directory"]
    json_path = os.path.join(out_path_root, JSON_FOLDER)
    date_str = datetime.strftime(datetime.now(), DATE_FMT)
    fpath = os.path.join(json_path, "%s.json" % date_str)

    # write json
    json_data = json.dumps(SELECTED_PAPERS)
    with open(fpath, "w+") as json_file:
        json_file.write(json_data)


# %% Main


def main():

    # load settings
    db_con = connect_to_database(DB_PATH)
    feeds = load_feeds(db_con)
    keywords = load_keywords(db_con)
    authors = load_authors(db_con)
    print(authors)

    if True:
        return
    load_settings()

    # Harvest papers
    paper_harvester()

    # Generate files
    fpath = generate_html_output()

    # Save as json
    save_as_json()

    # try to open in browser
    try:
        webbrowser.open_new_tab(fpath)
    except:
        pass

    return


# %% Execute
if __name__ == "__main__":
    main()
